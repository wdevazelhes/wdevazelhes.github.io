<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <title>William de Vazelhes</title>
</head>
<body>
    <header>
        <h2><a href="index.html">William de Vazelhes</a></h2>
    </header>

    <nav>
        <ul>
            <li><a href="index.html">About</a></li>            
            <li><a href="publications.html">Publications</a></li>
            <li><a href="talks.html">Talks</a></li>
            <li><a href="software.html">Code</a></li>
            <li><a href="cv.html">CV</a></li>
            <li><a href="blog.html">Blog</a></li>
        </ul>
    </nav>

    <section>
        <h2>Publications</h2>
      My publications can also be found on my <a href="https://scholar.google.com/citations?user=ple0xCwAAAAJ&hl=en">Google Scholar</a> webpage.<br>

<subsection>
    <h3>2025</h3>


    <ul>
        <li>
            <strong>ICML.</strong> Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees. <br>
            <emph><strong>William de Vazelhes</strong>, Xiao-Tong Yuan, Bin Gu</emph>
            <a href="https://arxiv.org/abs/2506.08558">[Paper]</a> <a href="https://github.com/wdevazelhes/2SP_icml2025">[Code]</a>
        </li>
    </ul>
</subsection>

<subsection>
    <h3>2024</h3>


    <ul>
        <li>
            <strong>TNNLS.</strong> Stagewise Training With Exponentially Growing Training Sets. <br>
            <emph>Bin Gu<sup>*</sup>, Hilal AlQuabeh<sup>*</sup>, <strong>William de Vazelhes<sup>*</sup></strong>, Zhouyuan Huo, Heng Huang</emph>
            <a href="https://ieeexplore.ieee.org/abstract/document/10542969/">[Paper]</a>
        </li>
        <li>
            <strong>IJCAI.</strong> Hard-Thresholding Meets Evolution Strategies in Reinforcement Learning. <br>
            <emph>Chengqian Gao<sup>*</sup>, <strong>William de Vazelhes<sup>*</sup></strong>, Hualin Zhang, Bin Gu, Zhiqiang Xu</emph>
            <a href="https://arxiv.org/abs/2405.01615">[Paper]</a>
        </li>
        <li>
            <strong>ICLR.</strong> New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions. <br>
            <emph>Xinzhe Yuan, <strong>William de Vazelhes</strong>, Bin Gu, Huan Xiong</emph>
            <a href="https://openreview.net/forum?id=fjf3YenThE">[Paper]</a> <a href="https://openreview.net/attachment?id=fjf3YenThE&name=supplementary_material">[Supplementary and Code]</a>
        </li>
        <li>
            <strong>AAAI.</strong> Iterative Regularization with k-support Norm: An Important Complement to Sparse Recovery. <br>
            <emph><strong>William de Vazelhes</strong>, Bhaskar Mukhoty, Xiao-Tong Yuan, Bin Gu <br></emph>
            <a href="https://arxiv.org/abs/2401.05394">[Paper]</a> <a href="https://github.com/wdevazelhes/IRKSN_AAAI2024">[Code]</a> <a href="files/poster_aaai.pdf" target="_blank">[Poster]</a> <a href="files/aaai_slides.pdf" target="_blank">[Slides]</a>
        </li>
        <li>
            <strong>AAAI.</strong> Limited Memory Online Gradient Descent for Kernelized Pairwise Learning with Dynamic Averaging. <br>
            <emph>Hilal AlQuabeh, <strong>William de Vazelhes</strong>, Bin Gu <br></emph>
        </li>
    </ul>

</subsection>

<subsection>
   <h3>2023</h3> 
   <ul>
    <li>
        <strong>NeurIPS.</strong> Direct Training of SNN using Local Zeroth Order Method. <br>
        <emph>Bhaskar Mukhoty<sup>*</sup>, Velibor Bojković<sup>*</sup>,<strong>William de Vazelhes</strong>, Xiaohan Zhao, Giulia De Masi, Huan Xiong, Bin Gu <br></emph>
        <a href="https://openreview.net/forum?id=eTF3VDH2b6">[Paper]</a> <a href="https://github.com/BhaskarMukhoty/LocalZO">[Code]</a>
    </li>
   </ul>
</subsection>

<subsection>
    <h3>2022</h3> 
    <ul>
        <li>
            <strong>NeurIPS.</strong> Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity. <br>  
        <emph><strong>William de Vazelhes</strong>, Hualin Zhang, Huimin Wu, Xiao-Tong Yuan, Bin Gu <br></emph>
            <a href="https://papers.nips.cc/paper_files/paper/2022/hash/8de5384f522efff26884559599c09312-Abstract-Conference.html">[Paper]</a> <a href="files/poster.pdf" target="_blank">[Poster]</a> <a href="https://slideslive.com/38991467">[Video]</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/8de5384f522efff26884559599c09312-Supplemental-Conference.zip">[Supplemental and Code]</a>
        </li>
        <li>
            <strong>ICDM.</strong> Efficient Semi-Supervised Adversarial Training without Guessing Labels. <br>
            <emph>Huimin Wu, <strong>William Vazelhes</strong>, Bin Gu <br></emph>
            <a href="https://ieeexplore.ieee.org/document/10027674">[Paper]</a>
        </li>
    </ul>
 </subsection>
 


 <subsection>
    <h3>2020</h3> 
    <ul>
        <li>
            <strong>JMLR.</strong> metric-learn: Metric Learning Algorithms in Python. <br>
            <emph><strong>William de Vazelhes</strong>, CJ Carey, Yuan Tang, Nathalie Vauquier, Aurélien Bellet <br></emph>
            <a href="https://github.com/scikit-learn-contrib/metric-learn">[Github Repo]</a> <a href="https://www.jmlr.org/papers/volume21/19-678/19-678.pdf">[Paper]</a> <a href="https://www.youtube.com/watch?v=uZhXRQD_flQ">[Video]</a>
        </li>
    </ul>
 </subsection>

 <small><sup>*</sup> denotes equal contribution. </small>

    </section>





</body>
</html>
