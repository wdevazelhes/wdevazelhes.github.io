<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <title>William de Vazelhes</title>
</head>
<body>
    <header>
        <h2>William de Vazelhes</h2>
    </header>

    <nav>
        <ul>
            <li><a href="index.html">About</a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="talks.html">Talks</a></li>
            <li><a href="software.html">Software</a></li>
            <li><a href="cv.html">CV</a></li>
        </ul>
    </nav>

    <div class="profile-img-container">
        <img src="images/idfull.jpeg" alt="Your Name">
    </div>

<header>
    <a href="https://twitter.com/willdvaz">Twitter</a> &#903; 
    <a href="https://mastodon.social/@wdevazelhes">Mastodon</a> &#903; 
    <a href="https://www.linkedin.com/in/wdevaz/">LinkedIn</a> &#903; 
    <a href="https://github.com/wdevazelhes">Github</a> &#903; 
    <a href="https://www.stackoverflow.com/users/5635843/william-de-vazelhes">StackOverflow</a> &#903; 
    <a href="https://scholar.google.com/citations?user=ple0xCwAAAAJ&hl=en">Google Scholar</a> 
</header>



    <section>
        <h2>About Me</h2>
        <p>Since 2021, I am a PhD candidate at <a href="https://mbzuai.ac.ae/">MBZUAI</a> in Machine Learning, in Abu Dhabi, UAE, under the supervision of Dr. <a href="https://jsgubin.github.io/">Bin Gu</a>, with an expected graduation date of <strong>May 2024</strong>. I am currently working on optimization for machine learning, in particular on zeroth-order optimization, and sparse learning. Before that, I graduated with an MSc. from <a href="https://en.wikipedia.org/wiki/Sup%C3%A9lec">Sup&eacute;lec</a> in 2016 (now part of <a href="https://en.wikipedia.org/wiki/CentraleSup%C3%A9lec">CentraleSupelec</a> and <a href="https://en.wikipedia.org/wiki/Paris-Saclay_University">Paris Saclay University</a>), and have worked at <a href="https://www.sidetrade.com/">Sidetrade</a> as a data scientist (2016-2017), at <a href="https://www.inria.fr/en">Inria</a>  in the <a href="https://team.inria.fr/magnet/">Magnet</a> Team as a research engineer (2017-2019), and at <a href="https://www.huawei.com/en/">Huawei</a> in the Paris Research and Algorithmic Lab as a PhD student (2019-2021).
        </p>
        <p align="center"><strong>My PhD thesis is available <a href="files/21010216_Vazelhes_Thesis.pdf" target="_blank">here</a>, as well as the <a href="files/defense.pdf" target="_blank">slides</a> and <a href="https://drive.google.com/file/d/132zndFq0rVwnhfSfSvAWvxUKiEDiiTI1/view?usp=sharing" target="_blank">video</a> from the defense.</strong></p>
    </section>
    <section>   
        <h2>Research Highlights</h2>
        <ul>
            <li>
                <strong>04/05/2024:</strong> Thesis manuscript submitted to MBZUAI Library! <a href="files/21010216_Vazelhes_Thesis.pdf" target="_blank">[PDF]</a>
            </li>
            <li>
                <strong>17/04/2024:</strong> Successfully defended my PhD thesis! <a href="files/defense.pdf" target="_blank">[Slides]</a> <a href="https://drive.google.com/file/d/132zndFq0rVwnhfSfSvAWvxUKiEDiiTI1/view?usp=sharing" target="_blank">[Video]</a>
            </li>
            <li>
                <strong>17/04/2024:</strong> Our paper <emph>"Hard-Thresholding Meets Evolution Strategies in Reinforcement Learning"</emph> got accepted at IJCAI, congratulations to the team! <a href="https://arxiv.org/abs/2405.01615">[Paper]</a>
            </li>
            <li>
                <strong>16/01/2024:</strong> Our paper <emph>"New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions"</emph> got accepted at ICLR, congratulations to Xinzhe Yuan! <a href="https://openreview.net/forum?id=fjf3YenThE">[Paper]</a> <a href="https://openreview.net/attachment?id=fjf3YenThE&name=supplementary_material">[Supplementary and Code]</a>
            </li>
            <li>
                <strong>09/12/2023:</strong> Our paper <emph>"Iterative Regularization with k-support Norm: An Important Complement to Sparse Recovery"</emph> got accepted at AAAI, congratulations to the team! <a href="https://arxiv.org/abs/2401.05394">[Paper]</a> <a href="https://github.com/wdevazelhes/IRKSN_AAAI2024">[Code]</a> <a href="files/poster_aaai.pdf" target="_blank">[Poster]</a> <a href="files/aaai_slides.pdf" target="_blank">[Slides]</a>
            </li>
            <li>
                <strong>09/12/2023:</strong> Our paper <emph>"Limited Memory Online Gradient Descent for Kernelized Pairwise Learning with Dynamic Averaging"</emph> got accepted at AAAI, congratulations to <a href="https://scholar.google.com/citations?user=_vbkrqMAAAAJ">Hilal AlQuabeh</a>!
            </li>
            <li><strong>21/09/2023:</strong> Our paper <emph>"Direct Training of SNN using Local Zeroth Order Method"</emph> got accepted at NeurIPS, congratulations to <a href="https://sites.google.com/view/bhaskar-mukhoty/home">Bhaskar Mukhoty</a> and <a href="https://scholar.google.com/citations?user=LXdhoooAAAAJ&hl=en">Velibor BojkoviÄ‡</a>! <a href="https://arxiv.org/abs/2401.05394">[Paper]</a> <a href="https://github.com/wdevazelhes/IRKSN_AAAI2024">[Code]</a></li>
            <li>
                <strong>11/05/2023:</strong> Successfully defended my PhD Candidacy Exam <a href="files/prez_candidacy.pdf" target="_blank">[Slides]</a> <a href="files/PhD_Candidacy_WdV.pdf" target="_blank">[PDF]</a>
            </li>
            <li><strong>08/10/2022:</strong> Got the NeurIPS 2022 Scholar Award (Funding for accomodation at Neurips 2022 for students accepted at NeurIPS).</li>
            <li>
                <strong>05/09/2022:</strong> Our paper on <emph>"Efficient Semi-Supervised Adversarial Training without Guessing Labels"</emph> got accepted at ICDM, congratulations to Huimin Wu! <a href="https://ieeexplore.ieee.org/document/10027674">[Paper]</a>
            </li>
            <li>
                <strong>15/07/2022:</strong> Our paper on <emph>"Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity"</emph> got accepted at NeurIPS, congratulations to the team! <a href="https://papers.nips.cc/paper_files/paper/2022/hash/8de5384f522efff26884559599c09312-Abstract-Conference.html">[Paper]</a> <a href="files/poster.pdf" target="_blank">[Poster]</a> <a href="https://slideslive.com/38991467">[Video]</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/8de5384f522efff26884559599c09312-Supplemental-Conference.zip">[Supplemental and Code]</a>
            </li>
        </ul>
    </section>


    <section><h2>Contact</h2>
        <ul>
            <li> wdevazelhes [at] gmail.com</li>
            <li> william.vazelhes [at] mbzuai.ac.ae</li>
        </ul>
    </section>

</body>

</html>
